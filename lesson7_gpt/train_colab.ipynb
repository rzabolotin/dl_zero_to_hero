{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e65acf64-1179-4012-9468-4c320ce99a91",
      "metadata": {
        "id": "e65acf64-1179-4012-9468-4c320ce99a91"
      },
      "source": [
        "# Тренируем большую модель на анекдотах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c14f5595-88fe-48cd-97ec-c1e8974c3135",
      "metadata": {
        "id": "c14f5595-88fe-48cd-97ec-c1e8974c3135"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/rzabolotin/dl_zero_to_hero/refs/heads/main/anecdots.txt"
      ],
      "metadata": {
        "id": "Kx2ALNeAi2UK",
        "outputId": "30ef6b5d-c996-435d-cf08-362687f25b71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Kx2ALNeAi2UK",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 18:52:01--  https://raw.githubusercontent.com/rzabolotin/dl_zero_to_hero/refs/heads/main/anecdots.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1578474 (1.5M) [text/plain]\n",
            "Saving to: ‘anecdots.txt.1’\n",
            "\n",
            "\ranecdots.txt.1        0%[                    ]       0  --.-KB/s               \ranecdots.txt.1      100%[===================>]   1.50M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-11-11 18:52:01 (23.1 MB/s) - ‘anecdots.txt.1’ saved [1578474/1578474]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1bc4f3d1-699b-4d75-a76b-c91ce56dfe10",
      "metadata": {
        "id": "1bc4f3d1-699b-4d75-a76b-c91ce56dfe10",
        "outputId": "f4d36b12-1121-4e62-93ee-59cb6b1593f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "??????????????????????????????? \n",
            "\n",
            "?О АВТОМАШИНАХ И ИХ ВОДИТЕЛЯХ ? \n",
            "\n",
            "??????????????????????????????? \n",
            "\n",
            "Несмотря на красный свет светофора, Луиза пересекла \n",
            "\n",
            "перекресток на своей машине и была остановлена полицейским. \n",
            "\n",
            "- Мадам, разве вы не видели красного света? - интересуется он. \n",
            "\n",
            "- Простите, мосье регулировщик, - отвечает она. - Красный \n",
            "\n",
            "свет я видела, а вот вас не заметила. \n",
            "\n",
            "Женщина-водитель после столкновения с другой машиной говорит: \n",
            "\n",
            "- Это моя вина! \n",
            "\n",
            "- Нет, мадам, - галантно отвечает \n"
          ]
        }
      ],
      "source": [
        "with open(\"anecdots.txt\", \"r\") as f_in:\n",
        "    book = f_in.read()\n",
        "print(book[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc1b670-c060-46c9-aadb-7b699b96f453",
      "metadata": {
        "id": "fdc1b670-c060-46c9-aadb-7b699b96f453"
      },
      "source": [
        "# Словарь и токенайзер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "4400bfd2-7e0d-4a26-80a0-a1326673b211",
      "metadata": {
        "id": "4400bfd2-7e0d-4a26-80a0-a1326673b211"
      },
      "outputs": [],
      "source": [
        "vocab = sorted(list(set(\"\".join(book))), key=lambda v: \"\\t\" if v == \".\" else v)\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "d4dbd67a-f4a3-476b-800d-0361598ab509",
      "metadata": {
        "id": "d4dbd67a-f4a3-476b-800d-0361598ab509"
      },
      "outputs": [],
      "source": [
        "char_to_index = {char: index for index, char in enumerate(vocab)}\n",
        "index_to_char = {index: char for char, index in char_to_index.items()}\n",
        "\n",
        "def tokenize(char):\n",
        "    return char_to_index.get(char, 0)\n",
        "\n",
        "def untokenize(index):\n",
        "    return index_to_char.get(index, \" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0173e5-077d-4daa-ab41-215362680b78",
      "metadata": {
        "id": "6d0173e5-077d-4daa-ab41-215362680b78"
      },
      "source": [
        "# Готовим данные для обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "71ac2e04-1983-474e-840f-665eaee5d9e7",
      "metadata": {
        "id": "71ac2e04-1983-474e-840f-665eaee5d9e7",
        "outputId": "cfe980be-998a-4b33-aa5e-a612f74dbe11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 29, 29,  ...,  3,  2,  1]) torch.Size([914848])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor([tokenize(x) for x in book], dtype=torch.long)\n",
        "print(data, data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "65cf69df-16bf-423f-82d2-9c1d9b179907",
      "metadata": {
        "id": "65cf69df-16bf-423f-82d2-9c1d9b179907"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "88f4a2b8-2cc2-4795-89fe-36590d3751e1",
      "metadata": {
        "id": "88f4a2b8-2cc2-4795-89fe-36590d3751e1"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, batch_size = 4):\n",
        "    data = val_data if split == \"valid\" else train_data\n",
        "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    X = torch.stack([data[i:i+block_size] for i in idx])\n",
        "    Y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
        "    X, Y = X.to(device), Y.to(device)\n",
        "    return(X,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66cd1ed3-0486-40d9-935d-d9393c0f6bb7",
      "metadata": {
        "id": "66cd1ed3-0486-40d9-935d-d9393c0f6bb7"
      },
      "source": [
        "# Функции полезные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f04e4af0-273a-49ed-ab4f-80018b74a0fb",
      "metadata": {
        "id": "f04e4af0-273a-49ed-ab4f-80018b74a0fb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model):\n",
        "    model.eval()\n",
        "    scores = {}\n",
        "    for split in ['train', 'valid']:\n",
        "        loss = 0\n",
        "        for i in range(n_eval):\n",
        "            X, Y = get_batch(split, batch_size=batch_size)\n",
        "            _, loss_i = model(X, Y)\n",
        "            loss += loss_i.item()\n",
        "        scores[split] = loss / n_eval\n",
        "    model.train()\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c126d83-240e-48bd-8b86-120b6100494c",
      "metadata": {
        "id": "3c126d83-240e-48bd-8b86-120b6100494c"
      },
      "source": [
        "# Константы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "ade6e697-3d1b-4a41-80e2-a97602c7219d",
      "metadata": {
        "id": "ade6e697-3d1b-4a41-80e2-a97602c7219d"
      },
      "outputs": [],
      "source": [
        "block_size = 256\n",
        "batch_size = 64\n",
        "dropout = 0.2\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "device = 'cuda'\n",
        "n_iter = 5000\n",
        "n_eval = 200\n",
        "learning_rate = 2e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b2d5e1-accd-452d-8fc8-82305f5f3f02",
      "metadata": {
        "id": "e1b2d5e1-accd-452d-8fc8-82305f5f3f02"
      },
      "source": [
        "# Модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f858fa96-a4ac-474a-8d2b-e55d319254bc",
      "metadata": {
        "id": "f858fa96-a4ac-474a-8d2b-e55d319254bc"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "43856bb1-fcad-4b0e-b06d-bcc0e8c3c655",
      "metadata": {
        "id": "43856bb1-fcad-4b0e-b06d-bcc0e8c3c655"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "2f19ac04-d4e7-40b0-a2d9-b3c90a68f73c",
      "metadata": {
        "id": "2f19ac04-d4e7-40b0-a2d9-b3c90a68f73c"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4a5604c5-4980-436d-a235-7da8a57c919c",
      "metadata": {
        "id": "4a5604c5-4980-436d-a235-7da8a57c919c"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1a93a802-bd96-444c-b0c9-013fc35e622e",
      "metadata": {
        "id": "1a93a802-bd96-444c-b0c9-013fc35e622e"
      },
      "outputs": [],
      "source": [
        "class Model7(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a996c9-8526-4ad6-9560-c4b49121be0f",
      "metadata": {
        "id": "56a996c9-8526-4ad6-9560-c4b49121be0f"
      },
      "source": [
        "# Тренировка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "dc38cdae-2957-4f3f-b9c4-8c8eaae8fe67",
      "metadata": {
        "id": "dc38cdae-2957-4f3f-b9c4-8c8eaae8fe67"
      },
      "outputs": [],
      "source": [
        "model7 = Model7()\n",
        "model7 = model7.to(device)\n",
        "optimizer7 = torch.optim.AdamW(model7.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model7.parameters())\n",
        "print(f\"The model has {total_params:,} trainable parameters.\")"
      ],
      "metadata": {
        "id": "YTjofketkBiZ",
        "outputId": "4ae760b1-216a-4540-e459-d780e30fb415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YTjofketkBiZ",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 10,849,680 trainable parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "089ba411-0422-481a-bfbb-c1b9e9f7419c",
      "metadata": {
        "id": "089ba411-0422-481a-bfbb-c1b9e9f7419c",
        "outputId": "2886e966-9481-460d-d864-78d159610162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss train: 2.5857, valid 2.6173\n",
            "Loss train: 1.9065, valid 1.9895\n",
            "Loss train: 1.5622, valid 1.7096\n",
            "Loss train: 1.3873, valid 1.5975\n",
            "Loss train: 1.2730, valid 1.5469\n",
            "1.2663553953170776\n",
            "CPU times: user 38min 54s, sys: 5min 47s, total: 44min 42s\n",
            "Wall time: 44min 51s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "m = model7\n",
        "o = optimizer7\n",
        "for i in range(n_iter):\n",
        "    X,Y = get_batch('train', batch_size=batch_size)\n",
        "    logits, loss = m(X, Y)\n",
        "    o.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    o.step()\n",
        "    if i%1000 == 0:\n",
        "        scores = evaluate_model(m)\n",
        "        print(f\"Loss train: {scores['train']:.4f}, valid {scores['valid']:.4f}\")\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "ae4785f1-8766-411d-86bf-241b2d6dced3",
      "metadata": {
        "id": "ae4785f1-8766-411d-86bf-241b2d6dced3",
        "outputId": "dfdc700a-13d1-498c-db28-ad0799e65dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". \n",
            "\n",
            "Фрамилиционер: \n",
            "\n",
            "- Эй, почему видчик. После бокальное как-то у церх время \n",
            "\n",
            "родильности, и кап ни осталось мне успела и кричил? \n",
            "\n",
            "Двести дорогулочку к учительницу. Ленит на карман и говорит: \n",
            "\n",
            "- Чем это ты теперь это куда? \n",
            "\n",
            "Вон отвечает: \n",
            "\n",
            "- Еще бы нет, а что с всегда он скакой еще раз выкирой? \n",
            "\n",
            "Полсуй, воды и купил рей салез горию образил в деревню: \n",
            "\n",
            "- Мне картоке, вы мне попал ударитовился прилить на дорожениями моих молодая? \n",
            "\n",
            "- Ну улыба то. У дали битгрись на радио в Эйстере, а порогулый \n",
            "\n",
            "\n",
            "Бильорард... слушайся. \n",
            "\n",
            "Утрой, если в госте по гослуге замуж. Туни вон незамужем: \n",
            "\n",
            "- Сейчас, что славу ничего цепь измененного глохневающего \n",
            "\n",
            "замерева! \n",
            "\n",
            "- Тогда как вы что, делжен никудал? \n",
            "\n",
            "- Хоть что муж приходит? \n",
            "\n",
            "- А зачем?! А! \n",
            "\n",
            "Муж \n",
            "\n",
            "Муж (обидо): \n",
            "\n",
            "- А как вы вылезаиваете, чтобы некоторить предлить своих \n",
            "\n",
            "салобных вздохом сегоднямие. \n",
            "\n",
            "Солдат-лист предстадавил она кого-нибудь, а жена, а \n",
            "\n",
            "когда может, ничего не верниться. \n",
            "\n",
            "Трамвается, приходит хлодок, где же и брачно за мяж. К\n"
          ]
        }
      ],
      "source": [
        "max_tokens = 1000\n",
        "prompt = torch.zeros([1, 1], dtype = torch.long)\n",
        "prompt = prompt.to(device)\n",
        "print(\"\".join([untokenize(x) for x in m.generate(prompt, max_tokens).tolist()[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "96d750e9-7c89-496f-a4ef-65787c5bab01",
      "metadata": {
        "id": "96d750e9-7c89-496f-a4ef-65787c5bab01",
        "outputId": "e53ece6a-e509-4f81-cadf-5b544524146b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss train: 1.1861, valid 1.5327\n",
            "Loss train: 1.1039, valid 1.5374\n",
            "Loss train: 1.0300, valid 1.5488\n",
            "Loss train: 0.9536, valid 1.5716\n",
            "Loss train: 0.8851, valid 1.5996\n",
            "1.0129832029342651\n",
            "CPU times: user 38min 48s, sys: 5min 45s, total: 44min 33s\n",
            "Wall time: 44min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "m = model7\n",
        "o = optimizer7\n",
        "for i in range(n_iter):\n",
        "    X,Y = get_batch('train', batch_size=batch_size)\n",
        "    logits, loss = m(X, Y)\n",
        "    o.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    o.step()\n",
        "    if i%1000 == 0:\n",
        "        scores = evaluate_model(m)\n",
        "        print(f\"Loss train: {scores['train']:.4f}, valid {scores['valid']:.4f}\")\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 1000\n",
        "prompt = torch.zeros([1, 1], dtype = torch.long)\n",
        "prompt = prompt.to(device)\n",
        "print(\"\".join([untokenize(x) for x in m.generate(prompt, max_tokens).tolist()[0]]))"
      ],
      "metadata": {
        "id": "Lm35kadss4kT",
        "outputId": "7a241278-900e-455f-cf34-440f1083a107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Lm35kadss4kT",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... \n",
            "\n",
            "- Ну тогда возьмен только и ничего не очень мясо. \n",
            "\n",
            "Попробовал через волкрасвенный вечер двинеет и спрашивает: \n",
            "\n",
            "- Как с кем не на грязнее, и скоро сильное время опытные запротив \n",
            "\n",
            "подзаки. \n",
            "\n",
            "Он вернулся на автобусе обычной коп. Потрезв возвращение с \n",
            "\n",
            "опуговорить в кладочкам светом плохо, сначала кофе. \n",
            "\n",
            "Очень спутился из разушения, стали урал. О, просыпаются \n",
            "\n",
            "перед соседней Смер Капендел Бри жены в родсти у другого \n",
            "\n",
            "кофе: \n",
            "\n",
            "- Ребята, два. Проклять ночью. \n",
            "\n",
            "- Вот когда делаешь, ты меня еще долго, когда то медовый месяц \n",
            "\n",
            "туда сам увидишь за ними угол казанычения. \n",
            "\n",
            "- И думаете, если мы уже хоть такая уж слышать, что из них \n",
            "\n",
            "стояли... \n",
            "\n",
            "Могилен и кричит: \n",
            "\n",
            "- Вообще-то насильно? \n",
            "\n",
            "Да но вот, где анего. \n",
            "\n",
            "Бросайтский следование: \n",
            "\n",
            "- Догоги наступите мне кот счастью задачную присантитутку, что на похороны \n",
            "\n",
            "дали. Он копрелывает мужик, на-у-ши пепчетали \"весе вешей\". \n",
            "\n",
            "- Да не все-та одина мужика - ежик и следы. \n",
            "\n",
            "- Блина! \n",
            "\n",
            "У мужиков-то стоит деджурные по путылке. \n",
            "\n",
            "- Дгай мясо ре\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model7, 'model7_full.pth')"
      ],
      "metadata": {
        "id": "njfXcnqbs7wP"
      },
      "id": "njfXcnqbs7wP",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7new = torch.load('model7_full.pth')\n",
        "model7new.to(device)\n",
        "model7new.eval()"
      ],
      "metadata": {
        "id": "lNadTsRAtseQ",
        "outputId": "b223d488-0c8f-4e48-ff90-afaf3570d193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lNadTsRAtseQ",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-abb3c2459eef>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model7new = torch.load('model7_full.pth')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model7(\n",
              "  (token_embedding_table): Embedding(144, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=144, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 1000\n",
        "prompt = torch.zeros([1, 1], dtype = torch.long)\n",
        "prompt = prompt.to(device)\n",
        "print(\"\".join([untokenize(x) for x in model7new.generate(prompt, max_tokens).tolist()[0]]))"
      ],
      "metadata": {
        "id": "AX7ocLUFt0xH",
        "outputId": "75b9fe23-486a-41e0-bf9c-f5abb0ab9050",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AX7ocLUFt0xH",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".А... \n",
            "\n",
            "- И я постанули. \n",
            "\n",
            "Турист: \n",
            "\n",
            "- А вот это до цыплянка по колге  живота? \n",
            "\n",
            "Ученица: \n",
            "\n",
            "- Семей выходи за ним в поле аистор, под рулем. \n",
            "\n",
            "Официант водит психиатру: \n",
            "\n",
            "- Я хочу подпить не могу на это старать радиаку выговаривать неосомнадцев, \n",
            "\n",
            "пока не заметил, что бы показать в своем слуга уюгом виски. \n",
            "\n",
            "- Он вчера в шитльнице говорит клубере. \n",
            "\n",
            "- Пожалуйста, - спрашивает официант у верность у клетки, - обиделся \n",
            "\n",
            "воскресенье нам не обратится к хмуровому сосед соседом. - Понял? \n",
            "\n",
            "- Да мы, где ввосполн! Вешали его на 26 лет. Последний дергает \n",
            "\n",
            "попугать в бутылку водки. \n",
            "\n",
            "- Ты чего будешь повезло, вышло замуж? - спрашивает официант. \n",
            "\n",
            "- Хорошо, что ты будешь великолепно. \n",
            "\n",
            "- Я спросил у своей жене, - заключенно сказал своему подруге: \n",
            "\n",
            "- У меня тебя были вычистыть свою дочь, я испытываю. \n",
            "\n",
            "- Вот испытываю шутиться за ти каждого круге. Это \n",
            "\n",
            "надоело доходаточно, такого же я сблява ее мужские сильнее. \n",
            "\n",
            "Врач опытанно несколько дней жилье, если ты не должно обрусить ни \n",
            "\n",
            "успевать себя ка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YLQ0ch3t764"
      },
      "id": "5YLQ0ch3t764",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}