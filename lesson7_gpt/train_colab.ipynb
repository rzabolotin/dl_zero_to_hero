{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65acf64-1179-4012-9468-4c320ce99a91",
   "metadata": {},
   "source": [
    "# Тренируем большую модель на анекдотах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c14f5595-88fe-48cd-97ec-c1e8974c3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bc4f3d1-699b-4d75-a76b-c91ce56dfe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "??????????????????????????????? \n",
      "\n",
      "?О АВТОМАШИНАХ И ИХ ВОДИТЕЛЯХ ? \n",
      "\n",
      "??????????????????????????????? \n",
      "\n",
      "Несмотря на красный свет светофора, Луиза пересекла \n",
      "\n",
      "перекресток на своей машине и была остановлена полицейским. \n",
      "\n",
      "- Мадам, разве вы не видели красного света? - интересуется он. \n",
      "\n",
      "- Простите, мосье регулировщик, - отвечает она. - Красный \n",
      "\n",
      "свет я видела, а вот вас не заметила. \n",
      "\n",
      "Женщина-водитель после столкновения с другой машиной говорит: \n",
      "\n",
      "- Это моя вина! \n",
      "\n",
      "- Нет, мадам, - галантно отвечает \n"
     ]
    }
   ],
   "source": [
    "with open(\"../anecdots.txt\", \"r\") as f_in:\n",
    "    book = f_in.read()\n",
    "print(book[:500])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1b670-c060-46c9-aadb-7b699b96f453",
   "metadata": {},
   "source": [
    "# Словарь и токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4400bfd2-7e0d-4a26-80a0-a1326673b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(\"\".join(book))), key=lambda v: \"\\t\" if v == \".\" else v)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4dbd67a-f4a3-476b-800d-0361598ab509",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {char: index for index, char in enumerate(vocab)}\n",
    "index_to_char = {index: char for char, index in char_to_index.items()}\n",
    "\n",
    "def tokenize(char):\n",
    "    return char_to_index.get(char, 0) \n",
    "\n",
    "def untokenize(index):\n",
    "    return index_to_char.get(index, \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0173e5-077d-4daa-ab41-215362680b78",
   "metadata": {},
   "source": [
    "# Готовим данные для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ac2e04-1983-474e-840f-665eaee5d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1, 29, 29,  ...,  3,  2,  1]) torch.Size([914848])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([tokenize(x) for x in book], dtype=torch.long)\n",
    "print(data, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cf69df-16bf-423f-82d2-9c1d9b179907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f4a2b8-2cc2-4795-89fe-36590d3751e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size = 4):\n",
    "    data = val_data if split == \"valid\" else train_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    Y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd1ed3-0486-40d9-935d-d9393c0f6bb7",
   "metadata": {},
   "source": [
    "# Функции полезные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04e4af0-273a-49ed-ab4f-80018b74a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, neval = 500):\n",
    "    model.eval()\n",
    "    scores = {}\n",
    "    for split in ['train', 'valid']:\n",
    "        loss = 0\n",
    "        for i in range(neval):\n",
    "            X, Y = get_batch(split, batch_size=batch_size)\n",
    "            _, loss_i = model(X, Y)\n",
    "            loss += loss_i.item()\n",
    "        scores[split] = loss / neval\n",
    "    model.train()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c126d83-240e-48bd-8b86-120b6100494c",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ade6e697-3d1b-4a41-80e2-a97602c7219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "device = 'cuda'\n",
    "n_iter = 5000\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b2d5e1-accd-452d-8fc8-82305f5f3f02",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f858fa96-a4ac-474a-8d2b-e55d319254bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43856bb1-fcad-4b0e-b06d-bcc0e8c3c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f19ac04-d4e7-40b0-a2d9-b3c90a68f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a5604c5-4980-436d-a235-7da8a57c919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a93a802-bd96-444c-b0c9-013fc35e622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model7(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a996c9-8526-4ad6-9560-c4b49121be0f",
   "metadata": {},
   "source": [
    "# Тренировка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc38cdae-2957-4f3f-b9c4-8c8eaae8fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Model7()\n",
    "optimizer7 = torch.optim.AdamW(model7.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "089ba411-0422-481a-bfbb-c1b9e9f7419c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m      4\u001b[0m     X,Y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m----> 5\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     o\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/eureka-yHAdeLJk/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/eureka-yHAdeLJk/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m, in \u001b[0;36mModel7.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# idx and targets are both (B,T) tensor of integers\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(idx) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/eureka-yHAdeLJk/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "m = model7\n",
    "o = optimizer7\n",
    "for i in range(n_iter):\n",
    "    X,Y = get_batch('train', batch_size=batch_size)\n",
    "    logits, loss = m(X, Y)\n",
    "    o.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    o.step()\n",
    "    if i%1000 == 0:\n",
    "        scores = evaluate_model(m)\n",
    "        print(f\"Loss train: {scores['train']:.4f}, valid {scores['valid']:.4f}\")\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae4785f1-8766-411d-86bf-241b2d6dced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      "\n",
      "А Двад педеет тей прудодашаецт \n",
      "\n",
      "ве. я мюне ветецу9ь? \n",
      "\n",
      "- Ва. Наютцише темнь дойси вл сссплас, ет то голикеня влолфем.. \n",
      "\n",
      "\n",
      "`атрднит яговат цедико! олу: \n",
      "\n",
      "- иломомы вазане\n",
      "\n",
      "- арт сту лоп$осоририн, ка былоно не дно де мубкана дить! - ноболиц к зСализы н вия \n",
      "\n",
      "-то Этогоскак трана. \n",
      "\n",
      "\n",
      "- Кирей.... А \n",
      "\n",
      "\n",
      "Раталь вождтане зилсью мает ирит три х го тсянашаны, чибер \n",
      "\n",
      "- баде? -пажит ! M \n",
      "\n",
      "- Геротасъевнь моетнодны, даслудинтсь бога сте доми сы тек?. \n",
      "\n",
      "- у дня рорулакт Мэтоко всенят стол мавабое ил и стенкол, полил, ту даме, нерол нае стичнлсодо - попрыж, вашеся ваке илокабел напрет. Ро Дасвамусть, я бро наживыг дризика рурнего аму педолавилал аю тогыпа ик и хосты готопромит, сэче \n",
      "\n",
      "добядатькриеня питеть восови- Шнонкув крудак окернаю наре-е! \n",
      "\n",
      "С? 1оне. \n",
      "\n",
      "- Вилени бищона чване шив иносо сяше Вый нит важоринт! ? Си сютска скук когали пол гю, дваховеб. Мь, нтвлиолевку. \n",
      "Са. \n",
      "\n",
      "- Дашрвохана, пи \n",
      "\n",
      "Жинилепе нужищ иена веннодотатилы чукомн оерворудийвосо те\" \n",
      "\n",
      "Ны кан. со \n",
      "\n",
      "- (у Гтаж звомачзжиши метамы \n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "prompt = torch.zeros([1, 1], dtype = torch.long)\n",
    "print(\"\".join([untokenize(x) for x in m.generate(prompt, max_tokens).tolist()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d750e9-7c89-496f-a4ef-65787c5bab01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
